\subsection{Cache Line} % or Cache Block
% https://en.wikipedia.org/wiki/CPU_cache#Cache_entries
%
% > On x86/x64, cache line is 64 bytes for many years now.
%      -- http://ithare.com/c-for-games-performance-allocations-and-data-locality/
% > In early caches these lines were 32 bytes long; nowadays the norm is 64 bytes.
%      -- Drepper, p. 15
% > It is not possible for a cache to hold partial cache lines.
%      -- Drepper, p. 16
% > A cache line is the "unit" of data you transfer to a cache.
%      -- http://www.cs.umd.edu/class/sum2003/cmsc311/Notes/Memory/introCache.html
\emph{Cache lines} or \emph{cache blocks} are the unit of data transfer between main
memory and cache.  They have a fixed size, which has been \say{64 bytes for many years} on
x86/x64 CPUs~\cites{ithare-paadl}[\href{https://youtu.be/WDIkqP4JbkE?t=21m41s}{21:41}]
{scott-meyers-talk}.%
\footnote{%
% > The original Pentium 4 processor also had an eight-way set associative L2 integrated
% > cache 256 KB in size, with 128-byte cache blocks.
%      -- https://en.wikipedia.org/wiki/CPU_cache#Example
% TODO: what about the block size of main memory?  Should it be the same?
Line sizes aren't \emph{necessarily} \alts{identical, homogenous} among a CPU's caches.
The Intel Pentium 4 processor had an \gls{l1d} cache with \say{64 bytes per cache
line}~\cite[p.~9]{pentium4} but an \gls{l2} cache with \say{128 bytes per cache
line}~\cite[p.~11]{pentium4}.}
\begin{comment}
   \multiplefootnoteseparator%
   % See <https://tex.stackexchange.com/a/71015>.
   \footnote{This can also be checked on the command line:
   \mintinline{bash}{cat /proc/cpuinfo | grep cache_alignment}}
\end{comment}
% > It means that as soon as you've accessed any single byte in a cache line, all the
% > other 63 bytes are already in L1 cache
%      -- http://ithare.com/c-for-games-performance-allocations-and-data-locality/
\alts{%
   This means accessing a single uncached 32-bit integer entails loading another 60
   adjacent bytes.,
   {This means when a single byte has to be loaded, another 63 adjacent bytes will be as
   well.},
   {When, for example, accessing a single byte that isn't already cached, another 63
   adjacent bytes will be loaded.}
}
% Even when compiling C++ for 64-bit systems, `int` is typically 32-bit.

My E-450 CPU is no exception and both of its data caches have 64-byte cache lines.%
\footnote{See \cref{app:cpuinfo}.}
% \begin{minted}[gobble=3]{bash}
%    $ getconf LEVEL1_DCACHE_LINESIZE; getconf LEVEL2_CACHE_LINESIZE
%    64
%    64
% \end{minted}
%stopzone
% \footnote{%
%    \mintinline{bash}!$ getconf LEVEL1_DCACHE_LINESIZE; getconf LEVEL2_CACHE_LINESIZE!\\
%    %stopzone
%    \noindent\mintinline{bash}!64!\\
%    \noindent\mintinline{bash}!64!
% }
We can verify this quite easily.  Consider \cref{lst:line-size}.  It loops over an array
with an increment given at compile time as \texttt{STEP} and measures the processor time.
\begin{center}
% \begin{listing}
   \inputminted[firstline=27]{c}{line-size/line-size.c}%
   % \caption{%
   \captionof{listing}{%
      Loop over \mintinline{text}{array} with increment \mintinline{text}{STEP}}
   \label{lst:line-size}
% \end{listing}
\end{center}
The results for different values of \texttt{STEP} are plotted in \cref{fig:line-size}.
% Starting from a step size of 16, the time roughly halves every time the step size is
% doubled.  For the first 4 step sizes however, it is almost constant.
As expected, the time roughly halves whenever the step size is doubled --- but only from a
step size of 16.  For the first 4 step sizes, it is almost constant.

% The reason why the loops take the same amount of time has to do with memory.  The
% running time of these loops is dominated by the memory accesses to the array, not by the
% integer multiplications.
%    -- https://igoro.com/archive/gallery-of-processor-cache-effects/
This is because the run times are \alts{primarily due to, dominated by} memory accesses.
Up to a step size of 8, every 64-byte line has to be loaded.  At 16, the values we modify
are 128 bytes apart,%
\footnote{16 \texttt{int64\_t} values of 8 bytes each}
so every other cache line is skipped.  At 32, three out of four cache lines are skipped,
and so on~\cite[cf.][example 2]{gallery}.

% Talk about how the term cache line is commonly conflated to mean an appropriately sized
% and *aligned* block of main memory that can be loaded into a cache line.  This is how it
% works, right?  Or can we load blocks starting at arbitrary addresses into a cache line?
% One could even imagine loading memory that isn't even contiguous.  Most sources don't
% seem to clear any of this up.  See <https://stackoverflow.com/q/3928995> ("How do cache
% lines work?")
\begin{comment}
   The term cache line is also used to refer to \alts{identically, appropriately} sized
   blocks in main memory.
   % that may be loaded into cache.
   These blocks are fixed:
   % I.e., each byte in main memory falls exactly into one cache line.
   % The boundaries

   Cache lines in main memory are fixed.
   % Data is not loaded starting from arbitrary addresses, but only from addresses that
   % are multiples of the cache line size.
   Data blocks that are loaded don't start at arbitrary addresses, but at multiples of the
   cache line size.
\end{comment}
% See <https://en.wikipedia.org/wiki/Partition_of_a_set>.  XXX: I feel like there's some
% kind of conflation of ideas going on here.
Both cache and main memory can be thought of as being partitioned%
% \footnote{%
%    In the set-theoretic sense
% }
\ (in the set-theoretic\x{al}\ sense) % https://en.wiktionary.org/wiki/set-theoretic
into \alts[3]{cache line-sized blocks, blocks of that size, cache lines}.  \alts[2]{{That
is, d}, D}ata is \alts{not, neither} \alts{read or written, loaded} starting from
arbitrary main memory addresses,
% ...nor is it loaded into arbitrarily aligned blocks in cache.
but only from addresses that are multiples of the cache line size.
% The starting address will by a multiple of the cache line size.

% TODO.  Part of the memory address partially determines which cache line is used.  The
% binary logarithm of the number of cache sets is the amount of bits required to identify
% the cache set.  The specific line used from that set depends on the replacement policy.
% A number of least significant (lowest-order, right-most) bits are ignored, since all
% addresses that only differ in those go into the same cache line.  If the line size is
% 64, log_2(64) = 6 bits are ignored.  See Drepper, page 15.

% TODO: this probably has some implications about aligning data.

% TODO: maybe talk about the 64-bit bus width and burst mode as well.  And maybe about how
% a write requires a read if we only write part of a cache line and the optimization of
% adding dummy writes (I think Andrei talked about this).
%
% [1]: https://stackoverflow.com/q/39182060 "Why isn't there a data bus which is as wide
%      as the cache line size?"

\begin{figure}
   \centering
   \input{tex/graphics/line-size-plot}
   \caption{Processor Times for Running \Cref{lst:line-size}}
   \label{fig:line-size}
\end{figure}

% vim: ft=tex tw=90 sts=-1 sw=3 et fdm=marker
