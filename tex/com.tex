\subsection{Cache-Oblivious Model}
%           Model-Oblivious Algorithms
%           Ideal-Cache Model
%           \Gls{com}

% This one's kind of nice (and Scott Meyers seems to think it has merit).  It also has a
% Wikipedia page, unlike the EMM, CMM, or IMM (a very rough index of practicality).

% This seems like FUD:
%
% > Typical cache-efficient algorithms require tuning to several cache parameters which
% > are not always available from the manufacturer and often difficult to extract
% > automatically.
%      -- http://erikdemaine.org/papers/BRICS2002/paper.pdf

% > In computing, a cache-oblivious algorithm (or cache-transcendent algorithm) is an
% > algorithm designed to take advantage of a CPU cache without having the size of the
% > cache (or the length of the cache lines, etc.) as an explicit parameter.
%      -- https://en.wikipedia.org/wiki/Cache-oblivious_algorithm
%

The \emph{\gls{com}} or \emph{ideal-cache model}, introduced by
\textcite{coa-for-publication}, \alts{concedes, leaves, forfeits} most of the
aforementioned problems to empirical evaluation and further increases the level of
abstraction.
%
Algorithms for the \gls{com}, called \emph{cache-oblivious algorithms}, are designed
without the \alts{
  cache size \(M\) or block size \(B\),
  values of \(M\) and \(B\),
}
as parameters.  This seems silly since these values typically can be queried easily
\x{automatically} at both run and compile time~\cite[50]{drepper2007}
% See <https://stackoverflow.com/a/7284876>.  Also, cache line sizes are super homogeneous
% anyway.
but, perhaps counterintuitively, models one aspect of memory hierarchies better than the
\gls{emm}.
% > This model was born out of the necessity to capture the hierarchical nature of memory
% > organization. [...] Although there have been other attempts to capture this
% > hierarchical information the cache oblivious model seems to be one of the most simple
% > and elegant ones.
%      -- Algorithms for Memory Hierarchies, page 194
%
% > The ideal cache oblivious model enables us to reason about a two level memory like the
% > external memory model but prove results about a multi-level memory model.
%      -- Algorithms for Memory Hierarchies, page 195
%
% > One consequence is that, if a cache-oblivious algorithm performs well between two
% > levels of the memory hierarchy (nominally called cache and disk), then it must
% > automatically work well between any two adjacent levels of the memory hierarchy.
%      -- http://erikdemaine.org/papers/BRICS2002/paper.pdf (erikcom)
%
% > Good algorithms for this model give us good algorithms for all values of B and M.
% > They are especially useful for multi-level caches.
%      -- https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-851-advanced-data-structures-spring-2012/calendar-and-notes/MIT6_851S12_L7.pdf
%
% > From a theoretical standpoint, the cache-oblivious model is appealing because it is
% > very clean.  A cache-oblivious algorithm is simply a RAM algorithm; it is only the
% > analysis that differs.  The cache-oblivious model also works well for multilevel
% > memory hierarchies, unlike the external-memory model, which only captures a two-level
% > hierarchy.
%      -- https://pdfs.semanticscholar.org/2f5a/76ccdc71971b746fbe7ff54db86c65a73e91.pdf
%
% > Frigo et al. showed that for many problems, an optimal cache-oblivious algorithm will
% > also be optimal for a machine with more than two memory hierarchy levels.
%      -- https://en.wikipedia.org/wiki/Cache-oblivious_algorithm
%
% > [W]e prove that an optimal cache-oblivious algorithm designed for two levels of memory
% > is also optimal for multiple levels.  We also prove that any optimal cache-oblivious
% > algorithm is also optimal in the previously studied HMM and SUMH models.
%      -- https://pdfs.semanticscholar.org/19ed/9795adc7204d3c9745b3c9f71f8496d26bb6.pdf
%
% > We prove that an optimal cache-oblivious algorithm designed for two levels of memory
% > is also optimal for multiple levels and that the assumption of optimal replacement in
% > the ideal-cache model can be simulated efficiently by LRU replacement.
%      -- http://supertech.csail.mit.edu/papers/FrigoLePr99.pdf#page=1, Abstract
%
An algorithm that performs well in the \gls{com} performs well across the entire memory
hierarchy~\cites[194\psq]{afmh}[4]{erikcom}; the same argument \alts[2]{\alts{for,
showing} asymptotically optimal movement of data, for data movement being
\x{asymptotically} \emph{optimal}} applies between any two levels of memory%
~\cite[lemma 15, \pno~10]{coa-paper}.
% TODO.  Is asymptotic analysis even useful, though?  The whole point of these models is
% that the hidden constants of "standard" asymptotic analysis matter.

% What does "optimal" mean?
%
% > An optimal cache-oblivious algorithm is a cache-oblivious algorithm that uses the
% > cache optimally (in an asymptotic sense, ignoring constant factors).
%      -- https://en.wikipedia.org/wiki/Cache-oblivious_algorithm
%
% > We say that [an algorithm] is *cache optimal* if the number of cache misses meet the
% > asymptotic lower bound for I/Os in the EMM for that problem.
%      -- Algorithms for Memory Hierarchies, page 181
%
% > [T]hese algorithms use an optimal amount of work and move data optimally among
% > multiple levels of cache.
%      -- http://supertech.csail.mit.edu/papers/FrigoLePr99.pdf#page=1, Abstract
%
% > For simplicity in this paper, we use the term "optimal" as a synonym for
% > "asymptotically optimal", since all our analyses are asymptotic.
%      -- http://supertech.csail.mit.edu/papers/FrigoLePr99.pdf#page=2, footnote
Optimal means that the asymptotic~\cite[2]{coa-paper} number of cache misses incurred
\x{by a cache-oblivious algorithm} matches the problem's lower bound in the \gls{com}.

% > In contrast to the external-memory model, algorithms in the cache-oblivious model
% > cannot explicitly manage the cache (issue block-read and block-write requests).  This
% > loss of freedom is necessary because the block and cache sizes are unknown.
%      -- http://erikdemaine.org/papers/BRICS2002/paper.pdf (erikcom), page 5
Cache misses take the place of \acrshortpl{io} \x{used in the \gls{emm}} because
cache-oblivious algorithms don't \alts{manage the cache, read or write cache lines}
explicitly.  This wouldn't be possible since the algorithms know neither the cache nor
the cache line size~\cite[5]{erikcom}.
% > The ideal cache uses the optimal off-line strategy of replacing the cache line whose
% > next access is furthest in the future
%      -- http://supertech.csail.mit.edu/papers/FrigoLePr99.pdf#page=1
Instead, the \gls{com} uses the optimal replacement strategy of evicting the cache line
that won't be accessed for the longest time in the future (Bélády's Algorithm).  This
\alts{
  is strangely \alts{at odds, out of touch, unrealistic} with real-world caches that don't
  know the future,
  {seems like out-of-touch, theoretical ivory-tower nonsense},
}.
% > The ideal-cache model makes the perhaps-questionable assumptions that there are only
% > two levels in the memory hierarchy, that memory is managed automatically by an optimal
% > cache-replacement strategy, and that the cache is fully associative.  We address these
% > assumptions in Section 6, showing that to a certain extent, these assumptions entail
% > no loss of generality.
%      -- http://supertech.csail.mit.edu/papers/FrigoLePr99.pdf#page=2
%
% > LRU and FIFO replacement do just as well as optimal replacement up to a constant
% > factor of memory transfers and up a constant factor wastage of the cache.
%      -- http://erikdemaine.org/papers/BRICS2002/paper.pdf#page=6 (erikcom)
%
% > The same argument extends to a variety of other replacement strategies.
%      -- http://supertech.csail.mit.edu/papers/Prokop99.pdf#page=46
%
% > We show that algorithms with regular complexity bounds (Equation (7.1)) (including all
% > algorithms heretofore presented) can be ported to less-ideal caches incorporating
% > least-recently-used (LRU) or first-in, first-out (FIFO) replacement policies [24, p.
% > 378].
%      -- http://supertech.csail.mit.edu/papers/Prokop99.pdf#page=51
%
% > [I]n many cases [the COM] is provably within a constant factor of a more realistic
% > cache's performance.
%      -- https://en.wikipedia.org/wiki/Cache-oblivious_algorithm
%
% > [A]s long as the number of memory transfers depends polynomially on the cache size M,
% > then halving M will only affect the running time by a constant factor.
%      -- http://erikdemaine.org/papers/BRICS2002/paper.pdf#page=6 (erikcom)
%
% > Intuitively, algorithms that slow down by a constant factor when memory (M) is reduced
% > to half, are called regular.
%      -- Algorithms for Memory Hierarchies, page 196
However, \citeauthor{coa-thesis} proves that for many algorithms\footnote{%
   Those satisfying equation (7.1) in~\cite[46]{coa-thesis}.  If the number of cache
   misses incurred by the algorithm only \say{depends polynomially on the cache size
   \(M\)}, the \alts{equation, condition} is satisfied~\cite[6]{erikcom}.
   % Halving the cache size only increases the number of cache misses by a constant
   % factor?
}
it only increases cache misses by a constant factor compared to various feasible
replacement strategies%
% ~\cite[lemma 12, \pno~10]{coa-paper}.%
% ~\cite[corollary 13, \pno~10]{coa-paper}.%
~\cite[corollary 19, \pno~46]{coa-thesis}.%
% See <http://supertech.csail.mit.edu/papers/Prokop99.pdf#page=46>.
\footnote{%
   e.g. LRU, FIFO, and random replacement
}

% According to `coa-paper` (page 9), the "four major assumptions" are:
% *  automatic replacement
% *  optimal replacement
% *  two levels of memory
% *  full associativity
%
% All simplifications and assumptions (I think):
% *  inherited/adopted from the EMM
%    *  two levels of memory
%    *  full associativity
%    *  tall cache
%       *  "It is also commonly assumed in external-memory algorithms."
%             -- http://erikdemaine.org/papers/BRICS2002/paper.pdf#page=7
%       *  I don't grok this one.
% *  new
%    *  automatic, optimal replacement
%    *  inclusion property
%       *  "[T]he values stored in cache i are also stored in cache i+1"
%             -- http://supertech.csail.mit.edu/papers/FrigoLePr99.pdf#page=10
%
% TODO: reiterate all simplifications and assumptions?

% > We show that the assumptions of two hierarchical memory models in the literature, in
% > which memory movement is programmed explicitly, are actually no weaker than ours.
% > Specifically, we prove (with only minor assumptions) that optimal cache-oblivious
% > algorithms in the ideal-cache model are also optimal in the hierarchical memory model
% > (HMM) [1] and in the serial uniform memory hierarchy (SUMH) model [5, 42].
%      -- http://supertech.csail.mit.edu/papers/Prokop99.pdf#page=12
%
% Exactly the same text is in the paper.  But only in the earlier version [1].
% [1]: https://pdfs.semanticscholar.org/19ed/9795adc7204d3c9745b3c9f71f8496d26bb6.pdf
%
% > An optimal cache-oblivious algorithm whose cache-complexity bound satisfies the
% > regularity condition (14) can be implemented optimally in expectation in multilevel
% > models with explicit memory management.
%      -- http://supertech.csail.mit.edu/papers/FrigoLePr99.pdf#page=11, theorem 17
%
% > [W]e have shown that optimal cache-oblivious algorithms in the ideal-cache model are
% > also optimal in the hierarchical memory model (HMM).
%      -- http://supertech.csail.mit.edu/papers/Prokop99.pdf#page=56
%
\citeauthor{coa-thesis} further justifies the model by proving, \say{with only minor
assumptions}~\cite[12]{coa-thesis}, that cache-oblivious algorithms that are optimal in
the \gls{com} \alts{can by executed with an optimal amount of \acrshortpl{io}, are also
optimal} in the \gls{emm}~\cite[theorem 32, \pno~56]{coa-thesis}.  In other words, most
cache-oblivious algorithms can be systematically transformed into \alts{cache-aware,
\gls{emm}} algorithms that asymptotically require the same amount of memory transfers in
the \gls{emm} as the cache-oblivious variant in the \gls{com}.

% TODO: given our motivation that "constant factors matter", what have we been doing here?

% TODO.  A regular algorithm that isn't explicitly for a hierarchical memory model
% (doesn't explicitly move data between different levels of memory) is already
% cache-oblivious, right?

% Time to give an example algorithm.  Options:
% *  matrix transposition
% *  matrix multiplication
% *  searching a tree (Van Emde Boas layout vs. standard layout)
% *  sorting
%    *  not easy
%    *  good performance seems problematic
%    *  looks like the autors of [1] dit it, though (lazy funnelsort)
% *  something about linked lists?
%
% Maybe matrix transposition...
%
% [1] https://app.cs.amherst.edu/~ccmcgeoch/cs34/papers/a2_2-brodal.pdf
\subsubsection{Cache-Oblivious Matrix Transposition}

The straightforward way to transpose \alts{
   an \(m\times n\) matrix \(D\),
   a matrix \(D\in\mathbb{R}^{m\times{}n}\),
}
out-of-place is to use two loops like so:
\begin{minted}[autogobble]{c}
   for (int i = 0; i < m; ++i)
      for (int j = 0; j < n; ++j)
         E[j][i] = D[i][j];
\end{minted}
Assuming \(D\) and \(E\) are stored in row-major layout (as they would be in the C and
\cpp{} languages), the reads from \(D\) are sequential memory accesses but the writes to
\(E\) are not.

\alts{When \(D^\mathsf{T}\) has, For} sufficiently long rows (\(m > B\))\footnote{%
   \(D^\mathsf{T}\) will be an \(n\times m\) matrix, so \(m\) is its number of columns.
},
every consecutive access will be to a different cache line.  If it has sufficiently many
rows,
\alts{{%
      the constant amount of lines that are still kept in cache by the optimal replacement
      strategy once the inner loop completes and we need them again becomes
      negligible.\x{\footnote{%
         With LRU or FIFO replacement, ...
   }}}, {
      no access will be to a cache line that is still loaded; once the inner loop
      completes, the cache line holding \alts[2]{\mintinline{c}{E[0][i]}, \(B_{0,i}\)}
      will have been evicted.%
      \footnote{%
         assuming LRU or FIFO replacement % XXX: but we use optimal replacement!
      }%
   },
}
Therefore, this algorithm incurs \(\Theta(mn)\) cache misses.

The algorithm is \alts{by definition, technically} already cache-oblivious since it
doesn't use \(M\) or \(B\), but it is not optimal\x{ in the \gls{com}}.  We can do better
with a divide-and-conquer approach.  The idea is to recursively divide the input matrix
\(D\) into two equal-sized submatrices along the greater dimension.  If \(m \geq n\) (more
rows than columns), let
\begin{equation*}
   D = \begin{bmatrix}
      D_1\\
      D_2
   \end{bmatrix}
\end{equation*}
and use \(D^\mathsf{T} = \begin{bmatrix} D_1^\mathsf{T} & D_2^\mathsf{T} \end{bmatrix}\)
to compute the transpose.  If \(m < n\), analogously \alts{slice, split} \(D\) vertically.
Eventually, the submatrices will become small enough so that pairs of input and output
submatrices fit into cache at the same time, at which point it doesn't matter in what
order we access the elements.\footnote{%
   No cache line will have to be evicted once loaded.
}
The recursion continues all the way down to \(1 \times 1\) submatrices, but this doesn't
change the theoretical analysis.  According to
\textcite[theorem 2 and 3, \ppno~19--21]{coa-thesis}
% \textcite[theorem 2 and 3, \ppno~19\psqq]{coa-thesis}
this algorithm incurs an optimal amount of \(\Theta(1 + mn / B)\) cache misses.
% This reduces the amount of cache misses to \(\Theta(1 + \nicefrac{mn}{B})\) according to
% \textcite[theorem 2, \ppno~19\psq]{coa-thesis}.
\Cref{lst:xpose} shows my implementation in C.

% FIXME: get rid of this `center` hack and get the vertical spacing around listings with
% captions to work in a better way.
\begin{center}
   % The code may only be 72 characters wide (78 when counting the indentation that will
   % be stripped).  Note that the array arguments will decay to pointers.
   \begin{minted}[autogobble, mathescape]{c}
      // Transpose the submatrix $(d_{ij})_{i\in I,\:j\in J}$.
      void transpose(int I[2], int J[2], int D[m][n], int E[n][m]) {
         int num_rows = 1 + I[1] - I[0];
         int num_cols = 1 + J[1] - J[0];
         if (num_cols == 1 && num_rows == 1) {
            E[J[0]][I[0]] = D[I[0]][J[0]];
         } else if (num_cols <= num_rows) {
            // Horizontally slice D into two submatrices and recurse.
            transpose((int[2]){I[0], I[0] + num_rows / 2 - 1}, J, D, E);
            transpose((int[2]){I[0] + num_rows / 2, I[1]}, J, D, E);
         } else { /* Vertically slice D analogously... */ }
      }
   \end{minted}
   % // Transpose the submatrix $(d_{ij}),\: i\in I, j\in J$.
   % // Transpose the submatrix $(d_{ij})_{I_0<i<I_1, J_0<j<J_1}$
   % // Transpose the submatrix $D\left[...\right]$
   \captionof{listing}{\acrshort{com}-Optimal Matrix Transposition}
   \label{lst:xpose}
\end{center}

% In practice, the new algorithm performs as badly as one might expect from this excessive
% use of recursion.
In practice, the new algorithm performs worse than the straightforward one for a lot of
matrix sizes before it eventually pulls ahead.
\alts{
   {\Cref{fig:xpose-speedup} gives the speedups for square matrices of various sizes},
   {\Cref{fig:xpose-speedup} gives the speedup compared to the straightforward
   implementation\x{, which is usually faster}},
}.
The problem may be the excessive use of recursion.

Kumar works around this by \say{[stopping] the recursion when the problem size becomes
less than a certain block size and then [using] the simple for loop implementation inside
the block}~\cite[199--201]{afmh}.  This seems to \alts{compromise, jeopardize} the
\alts{cache-obliviousness, cache-oblivious nature} of the algorithm.

Perhaps cache-oblivious algorithms should be seen as a starting point for further
optimizations that \alts{explicitly use the available information about the memory
hierarchy of the machine a program runs on, take the properties of the target machine into
account}.

\x{
Another approach may be to
\alts{
   {slice the matrix so that the split dimension becomes an even power of \(2\) for one of
   the submatrices},
   {\alts{split, divide} the matrix so that one of the resulting submatrices has a size
   that is an even power of \(2\) along the dimension we split}
}.
\alts{
   {Once a square submatrix whose dimensions are an even power of \(2\) is obtained, the
   identical order of traversal that would result from the recursive algorithm can be
   reproduced with a single loop.},
   {Once a submatrix whose dimensions both are an even power of \(2\) is obtained, the
   identical order of traversal that would result from the recursive algorithm can be
   reproduced with loops relatively easily.},
}
}

\begin{figure}[hbp]
   \centering
   \begin{tikzpicture}
      \datavisualization
      [scientific axes=clean,
       x axis={logarithmic,
               label={Matrix Size (\si{\mebi\byte})}, length=0.8\textwidth, ticks={
                  major at={1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024},
               },
              },
       y axis={include value=0, grid=at ticks, label={Speedup}, length=6cm,
         % ticks={major also at={1}}
       },
       visualize as scatter,
       scatter={style={mark=*, mark options={scale=.65}}}]
         data [read from file=xpose/speedup.csv, separator=\space];
   \end{tikzpicture}
   \captionsetup{width=.9\linewidth} % FIXME: hacks!
   \caption[Speedup Achieved by \acrshort{com}-Optimal Matrix Transposition]{Speedup
   Achieved by \acrshort{com}-Optimal Matrix Transposition; the CPU time used by the
   straightforward matrix transposition algorithm divided by that used by the optimal one}
   \label{fig:xpose-speedup}
\end{figure}

% > A cache-oblivious algorithm is simply a RAM algorithm; it is only the analysis that
% > differs.
%      -- https://pdfs.semanticscholar.org/2f5a/76ccdc71971b746fbe7ff54db86c65a73e91.pdf
%
% Algorithms are cache-oblivious by default.

% Work by Erik Demaine:
%
% [1]: http://erikdemaine.org/papers/BRICS2002/paper.pdf
%      "Cache-Oblivious Algorithms and Data Structures - 2002"
% [2]: https://web.archive.org/web/20160320051801/http://courses.csail.mit.edu/6.897/spring03/scribe_notes/L15/lecture15.pdf
%      "6.897: Advanced Data Structures - Spring 2003"
% [3]: https://pdfs.semanticscholar.org/2f5a/76ccdc71971b746fbe7ff54db86c65a73e91.pdf
%      "6.897: Advanced Data Structures - Spring 2005"
% [4]: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-851-advanced-data-structures-spring-2012/calendar-and-notes/MIT6_851S12_L7.pdf
%      "6.851: Advanced Data Structures - Spring 2012"

% vim: tw=90 sts=-1 sw=3 et fdm=marker
