\section{Types of CPU Caches}
% > Most modern desktop and server CPUs have at least three independent caches: an
% > instruction cache to speed up executable instruction fetch, a data cache to speed up
% > data fetch and store, and a translation lookaside buffer (TLB) used to speed up
% > virtual-to-physical address translation for both executable instructions and data.
%      -- https://en.wikipedia.org/wiki/CPU_cache#Overview
% > There are three common types of CPU caches: ...
%      -- Scott Meyers (talk at code::dive)
Current x86 CPUs \alts{generally, typically, commonly} have three main types of caches:
data caches, instruction caches, and \glspl{tlb}%
~\cite[\href{https://youtu.be/WDIkqP4JbkE?t=11m07s}{11:07}]{scott-meyers-talk}.
Some caches are used for data as well as instructions and are called \emph{unified}.%
~\cite[20]{drepper2007}.
\alts{{A processor may have multiple caches of each type, which}, {Multiple caches of each
type may be present, and}} are organised into numerical \emph{levels}
\alts{{starting at 1, the smallest and fastest level,},}
based on their size and speed.
% Each added level is bigger and slower than its predecessor.
% The smallest and fastest is level 1.

% TODO?  The reason to have multiple levels...

% > Often there are separate L1 caches for instructions and data
%      -- Algorithms for Memory Hierarchies, page 3
% > Systems nowaeays have at-least two levels of cache
%      -- Algorithms for Memory Hierarchies, page 172
% > [T]he caches from L2 on are unified caches which contain both code and data
%      -- Drepper, p. 31
% > Later Intel models have shared L2 caches for dual-core processors.  For quad-core
% > processors we have to deal with separate L2 caches for each pair of two cores.
%      -- Drepper, p. 35

% Terminology / Nomenclature.
In practice, a \alts{currently, presently} representative%
\footnote{%
   % https://en.wikipedia.org/wiki/Bobcat_(microarchitecture)
   E.g. for AMD Family 14h processors~\cite[30--32]{14h},
   % https://en.wikipedia.org/wiki/List_of_AMD_CPU_microarchitectures
   % https://en.wikipedia.org/wiki/Zen_(microarchitecture)
   % 32 KiB L1d, 64 KiB L1i, 512 KiB L2, 8 to 16 MiB L3
   AMD Zen (17h)~\cite{zen}, and
   % https://en.wikipedia.org/wiki/Kaby_Lake
   % https://en.wikipedia.org/wiki/Skylake_(microarchitecture)
   % 32 KiB L1d, 32 KiB L1i, 256 KiB L2, 2 to 8 MiB L3
   Intel Skylake desktop processors%
   ~\cite[figure 2-1, table 2-4]{skylake}
   % ~\cite[figure 2-1, \pno~2-2, table 2-4, \pno~2-6]{skylake}.
   % ~\cite[{2-2}, {2-6}]{skylake}.
   % <https://en.wikipedia.org/wiki/Bulldozer_(microarchitecture)> is too weird.
}
x86 cache hierarchy consists of:
\begin{itemize}
   % https://en.wikipedia.org/wiki/Cache_hierarchy#Shared_versus_private
   \item Separate level 1 data and instruction caches of 32 to 64 KiB for each core
      (denoted \gls{l1d} and \gls{l1i} by  \textcite[14--15]{drepper2007}).
      % TODO?  Why have a separate instruction cache?
      Machine instructions in \gls{l1i} are already decoded%
      ~\cite[31, 56]{drepper2007}.
      % ~\cite[14, 31, 56]{drepper2007}.
   % \item A level 2 cache for \say{both code and data}~\cite[31]{drepper2007}.
   \item A unified \gls{l2} cache of 256 to 512 KiB for each core.
   \item Often a unified \gls{l3} cache of 2 to 16 MiB shared between all cores.
   \item TODO: Some \glspl{tlb} I guess.
\end{itemize}

% \subsection{Access Times}
% http://ithare.com/infographics-operation-costs-in-cpu-clock-cycles/
% http://www.getitwriteonline.com/archive/040201hyphadj.htm
\alts{Estimates, Order-of-magnitude estimates} of typical access latencies \alts[2]{are as
follows, are given by \textcite{ithare-cycles}.}%
\footnote{%
   Intel~\cite[table 2-4]{skylake},
   \textcites
   % {ithare-paadl}{ithare-wisdoms}
   [\href{https://youtu.be/WDIkqP4JbkE?t=17m52s}{17:52}, slide 18]{scott-meyers-talk}
   [2--3, 171]{afmh}[16, 20--21]{drepper2007} all give comparable numbers for various
   architectures.
   % [\ppno~16, 20--21, fig. 3.10]{drepper2007}
}

% \begin{center}
%    \begin{tabular}{ r | c c c c }
%              & \gls{l1d} & \gls{l2} & \gls{l3} & Main Memory \\ \hline
%       Cycles & 3--4      & 10--12   & 30--70   & 100--150
%    \end{tabular}
% \end{center}
\begin{center}\begin{tabular}{ r c c c c }
   \toprule
          & \gls{l1d} & \gls{l2} & \gls{l3} & Main Memory \\
   \cmidrule[\lightrulewidth](l){2-5}
   % \midrule
   Cycles & 3--4      & 10--12   & 30--70   & 100--150 \\
   \bottomrule
\end{tabular}\end{center}

%
% These are taken from~\textcite{ithare-cycles} but comparable numbers are given by

% > [Instruction] cache is much less problematic than the data cache.
%      -- Drepper, p. 31
% TODO: move this paragraph?
The biggest target for optimizations is the data cache.  \say{[Instruction] cache is much
less problematic}~\cite[31]{drepper2007} and optimizations for data and instruction cache
tend to improve \gls{tlb} usage as well%
~\cite[\href{https://youtu.be/WDIkqP4JbkE?t=11m53s}{11:53}]{scott-meyers-talk}.

My laptop's AMD E-450 CPU has cores with \alts{an \gls{l1d} cache of 32 KiB, 32 KiB of
\gls{l1d} cache} and a unified \gls{l2} cache of 512 KiB each.%
\footnote{\Cref{app:cpuinfo} explains how to obtain this information.}
We can both verify these sizes and get \alts{a reasonably good measure, a rough measure,
an approximation} of the access times by profiling
\cref{lst:access-times}
% the \alts{program, listing} below
for different values of \mintinline{text}{SIZE}.%
% FIXME: the label is wrong: should be "Appendix" but is "Section".
\footnote{\Cref{app:cycles} details how.}
% It reads random memory addresses
% The program randomly reads
% locations
% The program repeatedly \alts{reads, accesses} random elements
This program repeatedly \alts{reads, accesses} elements
from \alts[2]{an array of the configured size, a thusly sized array}
in random \alts{order, sequence, succession}.
To do this
\alts{with minimal overhead, efficiently}, the array \alts{is first set up, acts} as a
circular, singly linked list where every element except the last \alts{points to a, has a}
random successor.  When compiled with \mintinline{text}{-DBASELINE}, only this
initialization is done.

\begin{comment}
   We will see why this measurement was conducted with random instead of sequential
   accesses in \cref{sec:prefetch}.

   \Cref{sec:prefetch} \alts{explains, will explain} the reason for \x{going through the
   trouble of} \alts{using, conducting} random accesses instead of just reading the array
   sequentially\x{for this measurement}.
\end{comment}
\alts{We use random accesses because,
      The reason for using random accesses is that}
the CPU will detect and optimize sequential access by a technique called
\emph{prefetching} discussed in \cref{sec:prefetch}, which would prevent us from
\alts{determining, assessing} access times.

% XXX: hacks!  Use the figure environment so that LaTeX won't display this listing after
% the plot showing the results of profiling it.  "LaTeX [only] keeps all floats of the
% same *type* in order" [1].  Use `\captionof` to label the listing correctly as a
% listing.
%
% [1]: https://tex.stackexchange.com/q/127742/#comment290982_127744
\begin{comment}
   \begin{figure}
      \inputminted[firstline=9]{c}{access-times/access-times.c}
      \captionof{listing}{TODO}
      \label{lst:access-times}
   \end{figure}
\end{comment}

% \newenvironment{code}{\captionsetup{type=listing}}{}
% \begin{code}
%    \inputminted[firstline=9]{c}{access-times/access-times.c}
%    \caption{TODO}
%    \label{lst:access-times}
% \end{code}

% XXX: hacks!  FIXME: I want to have the same amount of space after adding a caption with
% \captionof to a minted listing as there would be after a normal floating minted listing.
% See <https://tex.stackexchange.com/a/162074>.
% \captionsetup[listing]{aboveskip=5pt, belowskip=\baselineskip}

% Actually, don't use a float.  The listing should be allowed to span multiple pages which
% floats aren't.
%
% [1]: https://tex.stackexchange.com/q/14522/#comment484569_75880
% [2]: https://tex.stackexchange.com/q/175650
%      "How to allow page break inside a float environment?"
% [3]: https://tex.stackexchange.com/q/12428
\begin{center} % XXX: hack to get normal spacing after the caption and before the listing.
   \inputminted[firstline=12]{c}{access-times/access-times.c}
   % \captionof{listing}{}
   % \captionof{listing}{Randomly Access Elements of \mintinline{text}{array}}
   \captionof{listing}{Randomly Read Array Elements}
   % \captionof{listing}{Random Reads\label{lst:access-times}}
   \label{lst:access-times}
\end{center}

% https://tex.stackexchange.com/a/82473
% \global\csname @topnum\endcsname 0

% \Cref{fig:access-times} shows the \alts{difference, deltas} of CPU cycles used when and
% when not having defined \mintinline{text}{BASELINE}.
\Cref{fig:access-times} shows the extra CPU cycles used by \cref{lst:access-times}
\alts{in addition to, above, compared to, relative to} the \mintinline{text}{BASELINE}
version for different array sizes.
That is, only the cycles used by the main loop are \alts{given, counted}, not those for
initialization.  I divided by \mintinline{text}{N} to get the cycles spent \alts{per, on
each} loop iteration.

Up to 32 KiB, each access takes almost exactly 3 cycles.%
\footnote{The numerical results are shown in \vref{tab:access-times}.}
This is the \gls{l1d} access \alts{time, latency}.  At 32 KiB (the size of the \gls{l1d})
the time increases to about 3.4 cycles.  This is not surprising since the cache is shared
with other processes and the operating system, so some of our data gets \emph{evicted}.
The first dramatic increase happens at 64 KiB followed by smaller increases at 128 and 256
KiB.  I suspect we are seeing a mixture of \gls{l2} and \gls{l1d} accesses, with less and
less \gls{l1d} \emph{hits} and an \gls{l2} access time \x{of} around 25 cycles.

The values from 512 KiB (the size of the \gls{l2}) to 128 MiB \alts{exhibit, follow} a
similar pattern.  The relative increase when the array size matches that of the \gls{l2}
is \alts{greater, more striking} than for the \gls{l1d} before; possibly because \gls{l2}
is a unified cache that also holds instructions.  Eventually, more and more accesses go to
main memory, \alts{causing, incurring} delays of up to 200
cycles~\cite[cf.][\pno~17, figure 3.4]{drepper2007}.

% FOLDOC has an entry on 'working set'.
\alts[2]{{Evidently,}, The data suggests that} keeping the \emph{working set} a process
uses during a time interval small can yield dramatic performance improvements.

\tikzdatavisualizationset{
   array size vs cycles plot/.style={
      scientific axes=clean,
      x axis={
         logarithmic,
         ticks={major={
            /pgf/number format/int detect,
            at={
               2048 as 2, 8192 as 8, 32768 as \textbf{32}, 131072 as 128,
               524288 as \textbf{512}, 2097152 as \pgfmathprintnumber{2048},
               8388608 as \pgfmathprintnumber{8192},
               33554432 as \pgfmathprintnumber{32768},
               134217728 as \pgfmathprintnumber{131072},
               % $128\cdot 2^{10}$}}},
               % $2^{17}$}}},
         }}},
         grid={at={32768, 524288}},
         label={Array Size (KiB)},
         length=0.8\textwidth},
      y axis={include value=0, label={Cycles / Iteration}, length=6cm, grid=at ticks},
      visualize as scatter,
      scatter={style={mark=*, mark options={scale=.65}}},
   }
}

\begin{figure}
   \centering
   \tikz \datavisualization[array size vs cycles plot]
      data [read from file=access-times/access-times.csv, separator=\space];
   % \caption{Access Times for Random Reads (\Cref{lst:access-times})}
   \caption{Access Times for Random Reads}
   \label{fig:access-times}
\end{figure}

% vim: tw=90 sts=-1 sw=3 et fdm=marker
